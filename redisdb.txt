redis://h:pdd3c5f3259d816d177bf58b51a497e4dfb34f501943baa805a8ff6508187e341@ec2-52-55-98-249.compute-1.amazonaws.com:26399


a=get_links(q) 
b=[requests.get(i) for i in a] 
html_doc=[i.content.decode('utf-8') for i in b] 
soups=[BeautifulSoup(i, 'lxml') for i in html_doc] 
data=[i.select('script')[-2].text.split('_STATE__ = ')[-1].replace('\n', '').strip().rstrip(';') for i in soups] 
d2=[json.loads(i) for i in data]  
def loopi(i): 
        try: 
         a =   i['personStore']['name'] 
        except:     
         a = i['personStore']['person']['name'] 
        return a 
we=[loopi(i) for i in d2]    





def get_entries(list):
	entries=[list[i]['Entries'] for i in range(len(list))]
        return entries

def soup_it(item):
	soup=BeautifulSoup(i, 'lxml')
	return soup

def extract_json(i):  
         data=i.select('script')[-2].text.split('_STATE__ = ')[-1].replace('\n', '').strip().rstrip(';') 
         return json.loads(data)

def process_json(i):
	href=i['personStore']['meta']['seoCanonicalUrl']
	names=i['personStore']['name']
#	dob=i['personStore']['dateOfBirth'] 
#	dod=i['personStore']['dateOfDeath'] 
	loc=i['personStore']['location'] #deathloc
	t=i['personStore']['obituaries']  
	uv=[{'paperName':t[i]['gaSitename'], 'date':t[i]['dateCreated'], 'obituaryText':t[i]['obituaryText']} for i in range(len(t))]
	texts = {'texts':uv}
	condols=i['personStore']['guestBook']['condolences']['edges']   
	record={'href':href, 'name': names, 'location':loc, 'texts':texts, 'condolences': condols}
	return record


soups=[soup_it(i) for i in html_doc]
data= [extract_json(i) for i in soups]



d2=[json.loads(i) for i in data] 